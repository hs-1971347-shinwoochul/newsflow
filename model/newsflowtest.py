# -*- coding: utf-8 -*-
"""newsflowtest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16X2V50AsvQgHLjCHrXXmcIZTfJKxneMu
"""

!pip install torch torchvision torchaudio
!pip install pytorch-lightning
!pip install transformers
!pip install rouge-score
!pip install datasets
!pip install rouge_score

import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from transformers import AdamW, get_linear_schedule_with_warmup
from rouge_score import rouge_scorer
from datasets import load_metric
import torch
from tqdm.auto import tqdm

import json
from google.colab import drive

# 저장한 모델의 경로
model_save_path = '/content/drive/MyDrive/ColabNotebooks/NewsFlow(torch)/checkpoints/checkpoint-epoch=09-val_loss=1.09.ckpt'


class NewsFlowModel(pl.LightningModule):
    def __init__(self, tokenizer):
        super().__init__()
        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')
        self.tokenizer = tokenizer

    def forward(self, input_ids, attention_mask=None, labels=None):
        return self.model(input_ids, attention_mask=attention_mask, labels=labels)

    def generate(self, input_text):
      # 입력 텍스트를 텐서로 인코딩하고, 모델과 같은 디바이스로 이동
        raw_input_ids = self.tokenizer.encode(input_text, add_special_tokens=True)
        input_ids = torch.tensor([self.tokenizer.bos_token_id] + raw_input_ids + [self.tokenizer.eos_token_id])

     # 모델이 GPU에 있을 경우 텐서를 GPU로 이동
        if torch.cuda.is_available():
          input_ids = input_ids.to('cuda')

     # 요약 생성
        summary_ids = self.model.generate(input_ids.unsqueeze(0), num_beams=4, max_length=768, eos_token_id=1)
        return self.tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)



    def training_step(self, batch, batch_idx):
        outputs = self.forward(batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])
        loss = outputs.loss
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        outputs = self.forward(batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])
        val_loss = outputs.loss
        self.log("val_loss", val_loss)  # 검증 손실 로깅
        return {"val_loss": val_loss}

    def configure_optimizers(self):
        optimizer = AdamW(self.parameters(), lr=4e-5)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=0, num_training_steps=1000  # 필요에 따라 조정
        )
        return [optimizer], [scheduler]

# 모델과 토크나이저 생성
tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')
news_flow_model = NewsFlowModel(tokenizer)  # NewsFlowModel은 이미 정의된 것으로 가정합니다.

if torch.cuda.is_available():
    news_flow_model = news_flow_model.to('cuda')
# 모델의 상태 사전을 불러오기
news_flow_model = NewsFlowModel.load_from_checkpoint(checkpoint_path=model_save_path, tokenizer=tokenizer)

# 모델을 평가 모드로 설정
news_flow_model.eval()

import json

# JSON 파일에서 테스트 데이터 로드
try:
    with open('/content/drive/MyDrive/ColabNotebooks/NewsFlow(torch)/test_dataset(경제).json', 'r', encoding='utf-8') as file:
        test_data = json.load(file)

    # 데이터 타입과 길이 확인
    print("Data type:", type(test_data))
    print("Number of articles:", len(test_data))

    # 처음 1000개의 테스트 데이터만 사용
    test_data = test_data[:100]
    print("Number of articles used for test:", len(test_data))

except Exception as e:
    print("Error loading or processing data:", str(e))

# 데이터의 구조를 확인하는 코드
try:
    # 첫 번째 기사의 내용을 확인
    first_article_text = test_data[0]['text']
    print("First article text type:", type(first_article_text))
    print("First article text content:", first_article_text)

    # 첫 번째 문단의 첫 번째 문장을 확인
    first_sentence = first_article_text[0][0]['sentence']
    print("First sentence:", first_sentence)

except Exception as e:
    print("Error accessing data:", str(e))

from rouge_score import rouge_scorer
import numpy as np
from tqdm.auto import tqdm

# ROUGE 스코어 계산을 위한 객체 초기화
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
rouge_scores = []

# 테스트 데이터셋을 반복하면서 요약 생성 및 평가
for example in tqdm(test_data):
    # 각 기사의 모든 문장을 조합하여 하나의 텍스트로 만듭니다.
    # 빈 문단이 있을 수 있으므로 빈 리스트를 검사합니다.
    article_text = " ".join([sentence['sentence'] for paragraph in example['text'] if paragraph for sentence in paragraph])

    target_summary = example['title']  # 정답 요약으로 제목을 사용

    # 모델을 사용하여 요약 생성
    generated_summary = news_flow_model.generate(article_text)

    # 생성된 요약과 정답 요약 비교
    scores = scorer.score(target_summary, generated_summary)
    rouge_scores.append(scores)

# 평균 ROUGE 스코어 계산
average_scores = {
    'rouge1': np.mean([score['rouge1'].fmeasure for score in rouge_scores]),
    'rouge2': np.mean([score['rouge2'].fmeasure for score in rouge_scores]),
    'rougeL': np.mean([score['rougeL'].fmeasure for score in rouge_scores])
}

print("Average ROUGE Scores:", average_scores)

